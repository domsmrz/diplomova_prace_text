\chapter{Implementation}

\label{ch:implementation}

This chapter overviews the structure of the re-identification project implementation. For the detailed guide on how to use the attached software for \gls{iden} generation on a new dataset, please read \autoref{ch:guide}. For the implementation details, please refer to the attached code.

As we mentioned in the \autoref{sec:vl}, our whole work is designed as a module for \gls{VL}. Even though our work is largely independent of other modules within the framework of \gls{VL}, it shares one main component -- relation database. In this case, the database serves as an API between the modules and storage for the results.

In our case we worked with PostgreSQL as a management system. However, since we use SQLAlchemy library (\cite{sqlalchemy}) for all the communication with the database, we expect no compatibility issues with the usage of another management system.

\subsubsection*{Code Structure}
\todo[inline]{obrazok DB}
The implementation is split into several parts. Each part is independent of the others, using only the database as the common point (similarly to Videolytics modules). For the purpose of testing and evaluation, we also add a command-line interface for each part. With the database approach, we have two options on how to access the results. 
Alongside the parts we describe further in more detail,  we also provide several small scripts used for the evaluation in the attached code.

The first part -- feature vector part -- is responsible for creating feature vectors described in \autoref{ch:features}. The main pipeline for this module consists of downloading the data to compute the feature vectors from the database, annotate them with the feature vectors, and then upload the results back to the database. This process is used exactly as described for the histogram approaches. In the case of the neural networks, additionally, the data are downloaded for the training. In such a case, we first download the data, train, and save the network locally. Then we run the annotation process as described in the main pipeline with the trained network.

The second part -- clustering part -- is responsible for clustering the \glspl{det} (\autoref{ch:iden_construction}), that is for both -- trajectory generation and identity generation. For this process, the \glspl{det} from given cameras are again retrieved from the database along, if applicable, with the corresponding feature vectors created with the selected settings. If we use the construction of trajectories from before, they are also queried. Once the \glspl{det} are clustered according to the parameters, the resulting trajectories or \glspl{iden} are again committed to the database for use by other \gls{VL} modules. The options which were supplied for the \gls{iden} construction is also committed to the database as a new ``\gls{iden} model''. All the created \glspl{iden} are assigned to this model so they can be later queried if the same options are supplied.

\subsubsection*{Annotator}
We also provide a tool for manual annotation of the \glspl{det} we used to obtain the ground truth for our datasets. The tool queries the \glspl{det} from the database as well as selected \glspl{iden} or trajectory. User then can see the \glspl{det} frame-by-frame as well as their division to \glspl{iden}. It is possible to re-assign the wrongly assigned \glspl{det} to different \gls{iden}. This approach allows for semi-automated annotation: The \glspl{det} are firstly clustered by a (even poor) algorithm, then only wrongly \glspl{det} needs to be corrected. Once the re-assignment is complete, the resulting clustering can be committed back to the database as a new set of \glspl{iden} with a new ``identity model''.

\todo[inline]{screenshot z anotatoru}
